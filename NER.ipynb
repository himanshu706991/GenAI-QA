{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074ee876-b911-4dfb-9a79-8c4604865013",
   "metadata": {},
   "outputs": [],
   "source": [
    "Named Entity Recognition (NER) is a fundamental task in Natural Language Processing (NLP) that involves identifying and classifying entities in text into predefined categories such as names of people, organizations, locations, dates, etc. Hugging Face provides pre-trained models that can be used for NER tasks, and these models can also be fine-tuned on specific datasets if needed. Here's an end-to-end guide to building an NER system using Hugging Face.\n",
    "\n",
    "1. Environment Setup\n",
    "Install Necessary Libraries: Start by installing the required libraries.\n",
    "bash\n",
    "Copy code\n",
    "pip install transformers datasets torch\n",
    "2. Data Collection & Preprocessing\n",
    "Collect Data: If you're working on a domain-specific task, you might need a custom dataset. For general-purpose NER, you can use datasets like CoNLL-2003.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "Examine the Data: Take a look at the data structure to understand the format.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(dataset[\"train\"][0])\n",
    "Preprocess Data: Tokenize the input text and align the labels with the tokens.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "3. Model Selection\n",
    "Choose a Pre-Trained NER Model: You can use a pre-trained model like BERT, RoBERTa, or DistilBERT. For NER tasks, models fine-tuned on NER datasets are available.\n",
    "python\n",
    "Copy code\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=9)\n",
    "4. Fine-Tuning the Model\n",
    "Training Arguments: Define the training arguments for fine-tuning the model.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "Trainer Setup: Set up the Hugging Face Trainer for model training.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "5. Inference (NER)\n",
    "Running NER on New Text: Once trained, you can use the model to recognize named entities in new text.\n",
    "python\n",
    "Copy code\n",
    "from transformers import pipeline\n",
    "\n",
    "nlp_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "text = \"Hugging Face Inc. is a company based in New York.\"\n",
    "ner_results = nlp_ner(text)\n",
    "for entity in ner_results:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity']}\")\n",
    "6. Model Evaluation\n",
    "Evaluate the Model: Use metrics like precision, recall, and F1-score to evaluate the NER model.\n",
    "python\n",
    "Copy code\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "labels = tokenized_datasets[\"validation\"][\"labels\"]\n",
    "predictions = trainer.predict(tokenized_datasets[\"validation\"]).predictions\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "true_labels = [\n",
    "    [label_list[label] for label in label_seq if label != -100]\n",
    "    for label_seq in labels\n",
    "]\n",
    "true_predictions = [\n",
    "    [label_list[pred] for pred, label in zip(pred_seq, label_seq) if label != -100]\n",
    "    for pred_seq, label_seq in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "print(results)\n",
    "7. Deploying the NER System\n",
    "Deploy as a Web Service: Use Flask to create a simple API that allows users to perform NER on text inputs.\n",
    "python\n",
    "Copy code\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/ner\", methods=[\"POST\"])\n",
    "def ner():\n",
    "    text = request.json[\"text\"]\n",
    "    ner_results = nlp_ner(text)\n",
    "    return jsonify(ner_results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n",
    "8. Monitoring and Maintenance\n",
    "Monitor Performance: Track the model's performance in production, and fine-tune the model periodically as more data becomes available.\n",
    "Update the Model: Regularly retrain or fine-tune the model with new data to improve accuracy.\n",
    "9. Documentation and Sharing\n",
    "Document the System: Provide clear documentation for the entire pipeline, from data preprocessing to deployment.\n",
    "Share the Model: Optionally, share your fine-tuned model on the Hugging Face Model Hub.\n",
    "This guide provides a complete overview of building a Named Entity Recognition system using Hugging Face's tools. It covers everything from data preparation and model fine-tuning to deployment and evaluation, giving you a solid foundation for developing your own NER applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9155df0-30dc-451d-ad90-49c2b6683774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"conll2003\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da557a17-7115-4b15-9dee-86616141fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3042284a-f021-479b-9501-69a468b99835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0a0a56-7665-42f3-be3e-27590c5e1296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d247638-a883-461b-b389-fd3c905a59df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc00dd-07ea-4a95-b645-c48c1d88d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ef4f8f-59af-4450-bdca-e6c870f30df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "text = \"Hugging Face Inc. is a company based in New York.\"\n",
    "ner_results = nlp_ner(text)\n",
    "for entity in ner_results:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491b3b04-bb0d-475a-a04a-0a030c86c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "labels = tokenized_datasets[\"validation\"][\"labels\"]\n",
    "predictions = trainer.predict(tokenized_datasets[\"validation\"]).predictions\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "true_labels = [\n",
    "    [label_list[label] for label in label_seq if label != -100]\n",
    "    for label_seq in labels\n",
    "]\n",
    "true_predictions = [\n",
    "    [label_list[pred] for pred, label in zip(pred_seq, label_seq) if label != -100]\n",
    "    for pred_seq, label_seq in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5586a94-433a-4d74-a96a-397c413610d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/ner\", methods=[\"POST\"])\n",
    "def ner():\n",
    "    text = request.json[\"text\"]\n",
    "    ner_results = nlp_ner(text)\n",
    "    return jsonify(ner_results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8735c5bc-019f-4e77-bfe3-210002ea4377",
   "metadata": {},
   "source": [
    "8. Monitoring and Maintenance\n",
    "Monitor Performance: Track the model's performance in production, and fine-tune the model periodically as more data becomes available.\n",
    "Update the Model: Regularly retrain or fine-tune the model with new data to improve accuracy.\n",
    "9. Documentation and Sharing\n",
    "Document the System: Provide clear documentation for the entire pipeline, from data preprocessing to deployment.\n",
    "Share the Model: Optionally, share your fine-tuned model on the Hugging Face Model Hub.\n",
    "This guide provides a complete overview of building a Named Entity Recognition system using Hugging Face's tools. It covers everything from data preparation and model fine-tuning to deployment and evaluation, giving you a solid foundation for developing your own NER applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80c770e-95b2-471b-90ca-84be558520df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e4765a-965b-4dbe-8b5f-fc2ddeed3060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
