{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb73ba-0c3b-468a-9183-25a1ef0d91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Creating a Hybrid Retrieval-Augmented Generation (RAG) model involves several steps, including data preparation, implementing retrieval models (both dense and sparse), combining these models, and integrating a generative model like a transformer-based language model. Below is an outline of how you might implement this in Python using libraries like PyTorch, Hugging Face's Transformers, and FAISS.\n",
    "\n",
    "1. Environment Setup\n",
    "bash\n",
    "Copy code\n",
    "# Install necessary libraries\n",
    "pip install torch transformers faiss-cpu datasets\n",
    "2. Data Preparation\n",
    "You would need a dataset that includes documents or passages and a set of queries associated with those documents.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a sample dataset\n",
    "dataset = load_dataset('squad')\n",
    "documents = dataset['train']['context']  # List of documents/passages\n",
    "queries = dataset['train']['question']   # List of questions/queries\n",
    "3. Dense Retrieval Model (using Sentence Transformers)\n",
    "Dense retrieval involves encoding the queries and documents into dense vectors and finding the closest vectors.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load pre-trained model for dense retrieval\n",
    "dense_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Encode documents to create dense embeddings\n",
    "document_embeddings = dense_model.encode(documents, convert_to_tensor=True)\n",
    "\n",
    "# Encode a query for retrieval\n",
    "query = \"What is hybrid RAG?\"\n",
    "query_embedding = dense_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "# Perform dense retrieval\n",
    "hits = util.semantic_search(query_embedding, document_embeddings, top_k=5)\n",
    "top_docs = [documents[hit['corpus_id']] for hit in hits[0]]\n",
    "4. Sparse Retrieval Model (using TF-IDF with Scikit-Learn)\n",
    "Sparse retrieval can be done using traditional methods like TF-IDF.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorize documents using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Vectorize the query\n",
    "tfidf_query_vector = tfidf_vectorizer.transform([query])\n",
    "\n",
    "# Perform sparse retrieval\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarities = cosine_similarity(tfidf_query_vector, tfidf_matrix).flatten()\n",
    "top_indices = cosine_similarities.argsort()[-5:][::-1]\n",
    "top_sparse_docs = [documents[i] for i in top_indices]\n",
    "5. Combining Dense and Sparse Retrieval\n",
    "You can combine the results from both retrieval methods by either taking a union of the results or using a scoring mechanism to rank the combined results.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Combine results (naive union for simplicity)\n",
    "combined_docs = list(set(top_docs + top_sparse_docs))\n",
    "6. Generative Model for Augmentation\n",
    "After retrieving relevant documents, use a generative model to create a response based on these documents.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load a pre-trained generative model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# Concatenate the top documents as context\n",
    "context = \" \".join(combined_docs)\n",
    "\n",
    "# Generate a response based on the context\n",
    "input_ids = tokenizer.encode(context + \" \" + query, return_tensors='pt')\n",
    "outputs = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Decode the generated text\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "7. End-to-End Pipeline\n",
    "Finally, you can encapsulate this process into a function or a class for ease of use.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "class HybridRAG:\n",
    "    def __init__(self, dense_model, tfidf_vectorizer, gen_model, tokenizer, documents):\n",
    "        self.dense_model = dense_model\n",
    "        self.tfidf_vectorizer = tfidf_vectorizer\n",
    "        self.gen_model = gen_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.documents = documents\n",
    "        self.document_embeddings = dense_model.encode(documents, convert_to_tensor=True)\n",
    "        self.tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "    \n",
    "    def retrieve(self, query):\n",
    "        # Dense retrieval\n",
    "        query_embedding = self.dense_model.encode(query, convert_to_tensor=True)\n",
    "        dense_hits = util.semantic_search(query_embedding, self.document_embeddings, top_k=5)\n",
    "        top_dense_docs = [self.documents[hit['corpus_id']] for hit in dense_hits[0]]\n",
    "        \n",
    "        # Sparse retrieval\n",
    "        tfidf_query_vector = self.tfidf_vectorizer.transform([query])\n",
    "        cosine_similarities = cosine_similarity(tfidf_query_vector, self.tfidf_matrix).flatten()\n",
    "        top_sparse_indices = cosine_similarities.argsort()[-5:][::-1]\n",
    "        top_sparse_docs = [self.documents[i] for i in top_sparse_indices]\n",
    "        \n",
    "        # Combine retrievals\n",
    "        combined_docs = list(set(top_dense_docs + top_sparse_docs))\n",
    "        return combined_docs\n",
    "    \n",
    "    def generate(self, query):\n",
    "        combined_docs = self.retrieve(query)\n",
    "        context = \" \".join(combined_docs)\n",
    "        input_ids = self.tokenizer.encode(context + \" \" + query, return_tensors='pt')\n",
    "        outputs = self.gen_model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response\n",
    "\n",
    "# Instantiate the pipeline\n",
    "hybrid_rag = HybridRAG(dense_model, tfidf_vectorizer, model, tokenizer, documents)\n",
    "\n",
    "# Generate an answer\n",
    "query = \"What is hybrid RAG?\"\n",
    "response = hybrid_rag.generate(query)\n",
    "print(response)\n",
    "8. Running the Pipeline\n",
    "You can now use this pipeline to handle queries end-to-end, where it retrieves relevant information and generates a coherent response based on it.\n",
    "\n",
    "Summary\n",
    "This code provides a basic implementation of a Hybrid RAG model. You can extend and optimize this framework based on your dataset, use case, and computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37166dd-2a28-4851-8a18-1035b63b65c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c77f6197-f52c-45e0-91ef-5e7b6844482d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: datasets in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.21.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (70.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install torch transformers faiss-cpu datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3538ec17-18fe-4213-8158-1a5ed88362c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
