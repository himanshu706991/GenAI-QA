{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781d3eca-50cd-422a-8001-01bd1aa842e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Building a Text-to-Speech (TTS) and Speech Recognition system using Hugging Face involves working with pre-trained models and possibly fine-tuning them for specific tasks. Here's an end-to-end guide to creating both TTS and speech recognition systems.\n",
    "\n",
    "Part 1: Text-to-Speech (TTS)\n",
    "1. Environment Setup\n",
    "Install Necessary Libraries: Start by installing the required libraries.\n",
    "bash\n",
    "Copy code\n",
    "pip install transformers datasets torchaudio soundfile\n",
    "2. Model Selection\n",
    "Choose a Pre-Trained TTS Model: Hugging Face supports various TTS models like wav2vec2 and Tacotron2 through integration with the transformers and torchaudio libraries. For this example, we'll use a Tacotron2 model combined with WaveGlow for speech synthesis.\n",
    "python\n",
    "Copy code\n",
    "from transformers import Tacotron2ForTextToSpeech, AutoTokenizer\n",
    "from torchaudio.models import WaveGlow\n",
    "import torch\n",
    "\n",
    "tacotron2 = Tacotron2ForTextToSpeech.from_pretrained(\"nvidia/tacotron2\")\n",
    "waveglow = WaveGlow.from_pretrained(\"nvidia/waveglow\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/tacotron2\")\n",
    "3. Text Preprocessing\n",
    "Tokenize Input Text: Convert the input text into a format that the TTS model can understand.\n",
    "python\n",
    "Copy code\n",
    "text = \"Hugging Face is making NLP accessible to everyone.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "4. Generate Speech\n",
    "Generate Mel Spectrogram: Use the Tacotron2 model to generate a mel spectrogram from the input text.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "with torch.no_grad():\n",
    "    mel_outputs, mel_outputs_postnet, _, alignments = tacotron2.inference(inputs[\"input_ids\"])\n",
    "Convert to Audio Waveform: Pass the mel spectrogram through the WaveGlow model to generate the audio waveform.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "with torch.no_grad():\n",
    "    audio = waveglow.infer(mel_outputs_postnet)\n",
    "5. Save the Audio\n",
    "Save the Generated Audio: Use the soundfile library to save the audio file.\n",
    "python\n",
    "Copy code\n",
    "import soundfile as sf\n",
    "\n",
    "audio = audio.squeeze().cpu().numpy()\n",
    "sf.write(\"output.wav\", audio, 22050)\n",
    "6. Deploy the TTS System\n",
    "Deploy as a Web Service: Use Flask to create a simple API for generating speech from text.\n",
    "python\n",
    "Copy code\n",
    "from flask import Flask, request, send_file\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/tts\", methods=[\"POST\"])\n",
    "def tts():\n",
    "    text = request.json[\"text\"]\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    mel_outputs, mel_outputs_postnet, _, alignments = tacotron2.inference(inputs[\"input_ids\"])\n",
    "    audio = waveglow.infer(mel_outputs_postnet).squeeze().cpu().numpy()\n",
    "    sf.write(\"output.wav\", audio, 22050)\n",
    "    return send_file(\"output.wav\", as_attachment=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n",
    "Part 2: Speech Recognition\n",
    "1. Environment Setup\n",
    "Install Necessary Libraries: The same setup as TTS, but you'll focus on speech recognition models like wav2vec2.\n",
    "bash\n",
    "Copy code\n",
    "pip install transformers datasets torchaudio\n",
    "2. Model Selection\n",
    "Choose a Pre-Trained Speech Recognition Model: Hugging Face provides the wav2vec2 model for speech-to-text tasks.\n",
    "python\n",
    "Copy code\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "3. Load and Preprocess Audio\n",
    "Load an Audio File: Load a speech audio file for transcription.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "speech_array, sampling_rate = torchaudio.load(\"path_to_audio.wav\")\n",
    "Resample the Audio: Resample the audio to the model's required sampling rate.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "resampler = torchaudio.transforms.Resample(sampling_rate, 16000)\n",
    "speech = resampler(speech_array).squeeze().numpy()\n",
    "Tokenize the Audio: Tokenize the audio input for the model.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "inputs = processor(speech, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "4. Run Speech Recognition\n",
    "Generate Transcription: Use the wav2vec2 model to transcribe the audio to text.\n",
    "python\n",
    "Copy code\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.decode(predicted_ids[0])\n",
    "print(transcription)\n",
    "5. Deploy the Speech Recognition System\n",
    "Deploy as a Web Service: Use Flask to create a simple API for converting speech to text.\n",
    "python\n",
    "Copy code\n",
    "from flask import Flask, request, jsonify\n",
    "import torchaudio\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/stt\", methods=[\"POST\"])\n",
    "def stt():\n",
    "    audio_file = request.files[\"audio\"]\n",
    "    speech_array, sampling_rate = torchaudio.load(audio_file)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    inputs = processor(speech, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "    return jsonify({\"transcription\": transcription})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n",
    "Part 3: Monitoring and Maintenance\n",
    "Monitor Performance: Track the performance of your TTS and Speech Recognition systems with real-time monitoring and user feedback.\n",
    "Fine-Tuning: As more data becomes available, fine-tune the models to improve performance on your specific use case.\n",
    "Part 4: Documentation and Sharing\n",
    "Document the Process: Provide documentation on how to use, maintain, and extend the TTS and Speech Recognition systems.\n",
    "Share the Models: Share the trained models and code on platforms like GitHub or Hugging Face Model Hub.\n",
    "This guide gives you a comprehensive approach to building and deploying both TTS and Speech Recognition systems using Hugging Face's NLP models and the torchaudio library. These systems can be customized further based on specific needs and use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f9437-5437-4a63-8eda-22304317db37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c1100d-ba79-4168-917b-475e8e0536aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.21.0)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.4.0-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Collecting soundfile\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Collecting torch==2.4.0 (from torchaudio)\n",
      "  Using cached torch-2.4.0-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.4.0->torchaudio) (4.12.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.4.0->torchaudio) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.4.0->torchaudio) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.4.0->torchaudio) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.4.0->torchaudio) (70.0.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch==2.4.0->torchaudio) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch==2.4.0->torchaudio) (1.3.0)\n",
      "Downloading torchaudio-2.4.0-cp312-cp312-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.2/2.4 MB 5.3 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.6/2.4 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.0/2.4 MB 7.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.4/2.4 MB 8.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 8.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.2/2.4 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 8.1 MB/s eta 0:00:00\n",
      "Using cached torch-2.4.0-cp312-cp312-win_amd64.whl (197.8 MB)\n",
      "Downloading soundfile-0.12.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.3/1.0 MB 8.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 0.7/1.0 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 8.0 MB/s eta 0:00:00\n",
      "Installing collected packages: torch, soundfile, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.3.1\n",
      "    Uninstalling torch-2.3.1:\n",
      "      Successfully uninstalled torch-2.3.1\n",
      "Successfully installed soundfile-0.12.1 torch-2.4.0 torchaudio-2.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Himanshu Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~-rch'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.18.1 requires torch==2.3.1, but you have torch 2.4.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torchaudio soundfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcff6cc-0078-40b5-8397-2f029f9ce963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Tacotron2ForTextToSpeech, AutoTokenizer\n",
    "from torchaudio.models import WaveGlow\n",
    "import torch\n",
    "\n",
    "tacotron2 = Tacotron2ForTextToSpeech.from_pretrained(\"nvidia/tacotron2\")\n",
    "waveglow = WaveGlow.from_pretrained(\"nvidia/waveglow\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/tacotron2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c7a79-4004-4993-86ba-acec1811e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hugging Face is making NLP accessible to everyone.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d914c-bbec-45a2-89a5-8e853e615ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    mel_outputs, mel_outputs_postnet, _, alignments = tacotron2.inference(inputs[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e020fd9c-9557-417b-b229-65bdaf79a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    audio = waveglow.infer(mel_outputs_postnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc3677-8d78-447a-8381-6ef62a6b2e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "audio = audio.squeeze().cpu().numpy()\n",
    "sf.write(\"output.wav\", audio, 22050)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97844fa0-9c27-468e-baf9-5182faebd91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, send_file\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/tts\", methods=[\"POST\"])\n",
    "def tts():\n",
    "    text = request.json[\"text\"]\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    mel_outputs, mel_outputs_postnet, _, alignments = tacotron2.inference(inputs[\"input_ids\"])\n",
    "    audio = waveglow.infer(mel_outputs_postnet).squeeze().cpu().numpy()\n",
    "    sf.write(\"output.wav\", audio, 22050)\n",
    "    return send_file(\"output.wav\", as_attachment=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b41a5d-8b2d-40ae-9351-cbcab82ab3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c72c0b-47ea-49a2-821a-07131933e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6affb3df-9f52-4dc1-b39f-5c6467248139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d625b-9378-48a8-aef2-9c6f0a8dfb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cd4c7a-9c22-4f6a-a767-d0b312826013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27cc279-052b-4846-a3f3-cce8c14028fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_array, sampling_rate = torchaudio.load(\"path_to_audio.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e4251d-5adb-4065-b22c-6aaaf1eeb76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampler = torchaudio.transforms.Resample(sampling_rate, 16000)\n",
    "speech = resampler(speech_array).squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fe1d14-a09a-469c-bda5-6e757b550fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(speech, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfcfedb-b057-40cf-81ea-77ac62323960",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.decode(predicted_ids[0])\n",
    "print(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f726e8-61b2-4323-a04f-898da6ef6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import torchaudio\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/stt\", methods=[\"POST\"])\n",
    "def stt():\n",
    "    audio_file = request.files[\"audio\"]\n",
    "    speech_array, sampling_rate = torchaudio.load(audio_file)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    inputs = processor(speech, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "    return jsonify({\"transcription\": transcription})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c8e9f4-2c5c-4a83-9b13-9f3f9b5df7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "art 3: Monitoring and Maintenance\n",
    "Monitor Performance: Track the performance of your TTS and Speech Recognition systems with real-time monitoring and user feedback.\n",
    "Fine-Tuning: As more data becomes available, fine-tune the models to improve performance on your specific use case.\n",
    "Part 4: Documentation and Sharing\n",
    "Document the Process: Provide documentation on how to use, maintain, and extend the TTS and Speech Recognition systems.\n",
    "Share the Models: Share the trained models and code on platforms like GitHub or Hugging Face Model Hub.\n",
    "This guide gives you a comprehensive approach to building and deploying both TTS and Speech Recognition systems using Hugging Face's NLP models and the torchaudio library. These systems can be customized further based on specific needs and use cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
