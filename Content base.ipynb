{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a5546-0da2-495e-8690-d77ce0d16261",
   "metadata": {},
   "outputs": [],
   "source": [
    "Building a content recommendation system using NLP with Hugging Face involves several steps, from data preparation to model deployment. Hereâ€™s a comprehensive guide to creating an end-to-end content recommendation system.\n",
    "\n",
    "1. Environment Setup\n",
    "Install Necessary Libraries: Start by installing the required libraries.\n",
    "bash\n",
    "Copy code\n",
    "pip install transformers datasets torch scikit-learn\n",
    "2. Data Collection & Preprocessing\n",
    "Collect Data: You'll need a dataset that includes user interactions with content. For instance, the MovieLens dataset can be used for movie recommendations.\n",
    "Load Data: Load the dataset using Pandas or the datasets library.\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"path_to_dataset.csv\")\n",
    "# Assuming the dataset has columns: user_id, item_id, and interaction (e.g., rating)\n",
    "Preprocess Data: Prepare the data for model training, including tokenization and text vectorization.\n",
    "python\n",
    "Copy code\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Example: Assuming 'item_description' is the text to be used for recommendations\n",
    "data['tokenized'] = data['item_description'].apply(lambda x: tokenize_text(x)['input_ids'])\n",
    "3. Feature Engineering\n",
    "Content-Based Features: Extract features from the text content. Use a pre-trained BERT model to get embeddings of content descriptions.\n",
    "python\n",
    "Copy code\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "data['embeddings'] = data['tokenized'].apply(lambda x: model(x)['last_hidden_state'].mean(dim=1).detach().numpy())\n",
    "User-Based Features: Aggregate content embeddings for users based on their interaction history.\n",
    "python\n",
    "Copy code\n",
    "user_embeddings = data.groupby('user_id')['embeddings'].apply(lambda x: x.mean(axis=0))\n",
    "4. Model Selection\n",
    "Choose a Recommendation Model: Use a simple nearest neighbors model or a more advanced model like matrix factorization or neural collaborative filtering. Here, we'll use a simple nearest neighbors model.\n",
    "python\n",
    "Copy code\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
    "knn.fit(np.stack(data['embeddings'].values))\n",
    "5. Model Training (Optional)\n",
    "Fine-Tuning a Pre-Trained Model: If you want to fine-tune the BERT model on your specific data, you can do so using the Hugging Face Trainer API.\n",
    "python\n",
    "Copy code\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data['tokenized'],  # Modify according to your dataset format\n",
    "    eval_dataset=validation_data['tokenized'],  # Modify according to your dataset format\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "6. Content Recommendation\n",
    "Generate Recommendations: For a given user, find the nearest items based on content embeddings.\n",
    "python\n",
    "Copy code\n",
    "user_id = 123  # Example user ID\n",
    "user_embedding = user_embeddings.loc[user_id]\n",
    "distances, indices = knn.kneighbors([user_embedding])\n",
    "\n",
    "recommended_items = data.iloc[indices[0]]['item_id']\n",
    "print(f\"Recommended items for user {user_id}: {recommended_items.tolist()}\")\n",
    "7. Model Evaluation\n",
    "Evaluate the Recommendations: Use metrics like precision, recall, or mean reciprocal rank (MRR) to evaluate the quality of recommendations.\n",
    "python\n",
    "Copy code\n",
    "def precision_at_k(actual, predicted, k):\n",
    "    return len(set(predicted[:k]) & set(actual)) / k\n",
    "\n",
    "# Example evaluation\n",
    "actual_items = data[data['user_id'] == user_id]['item_id'].tolist()\n",
    "predicted_items = recommended_items.tolist()\n",
    "print(f\"Precision at 5: {precision_at_k(actual_items, predicted_items, 5)}\")\n",
    "8. Model Deployment\n",
    "Save the Model: Save the trained model and tokenizer for deployment.\n",
    "python\n",
    "Copy code\n",
    "import joblib\n",
    "\n",
    "joblib.dump(knn, \"knn_model.pkl\")\n",
    "tokenizer.save_pretrained(\"./tokenizer\")\n",
    "model.save_pretrained(\"./bert_model\")\n",
    "Deploy the Recommendation System: Deploy the system using Flask, FastAPI, or any other web framework.\n",
    "python\n",
    "Copy code\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "knn_model = joblib.load(\"knn_model.pkl\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./tokenizer\")\n",
    "model = AutoModel.from_pretrained(\"./bert_model\")\n",
    "\n",
    "@app.route(\"/recommend\", methods=[\"POST\"])\n",
    "def recommend():\n",
    "    user_id = request.json['user_id']\n",
    "    user_embedding = user_embeddings.loc[user_id]\n",
    "    distances, indices = knn_model.kneighbors([user_embedding])\n",
    "    recommended_items = data.iloc[indices[0]]['item_id'].tolist()\n",
    "    return jsonify({\"recommended_items\": recommended_items})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n",
    "9. Monitoring and Maintenance\n",
    "Monitor Recommendations: Keep track of recommendation performance using real-time analytics.\n",
    "Update the Model: Periodically update the embeddings and retrain the nearest neighbors model as new content is added.\n",
    "10. Documentation and Sharing\n",
    "Document the Process: Provide documentation for the entire pipeline from data preprocessing to deployment.\n",
    "Share the Model and Code: Optionally, share the model and code on platforms like GitHub or Hugging Face Model Hub.\n",
    "This guide gives you a complete overview of building a content recommendation system using NLP and Hugging Face tools, from data processing to model deployment. The approach can be customized based on your specific use case, data availability, and desired complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
