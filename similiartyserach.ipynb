{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beb2dd5-7d02-4d03-8b9f-f60b455cbae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Use Case: Similarity Search Across Multiple PDF Documents\n",
    "In this use case, we will build a similarity search system to retrieve relevant sections or documents from multiple PDF files based on a user query. We’ll use Hugging Face Transformers to generate embeddings for text and Vector DBs to perform the similarity search.\n",
    "\n",
    "End-to-End Pipeline\n",
    "Extract Text from PDFs\n",
    "Generate Embeddings\n",
    "Store Embeddings in a Vector Database\n",
    "Query Processing and Similarity Search\n",
    "Retrieve and Display Results\n",
    "1. Extract Text from PDFs\n",
    "We'll use the PyMuPDF library to extract text from PDF files.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdfs(pdf_paths):\n",
    "    texts = []\n",
    "    for path in pdf_paths:\n",
    "        pdf_document = fitz.open(path)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_document)):\n",
    "            page = pdf_document.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "pdf_paths = [\"document1.pdf\", \"document2.pdf\"]\n",
    "pdf_texts = extract_text_from_pdfs(pdf_paths)\n",
    "2. Generate Embeddings\n",
    "Use Hugging Face Transformers to convert text into embeddings.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained model from Hugging Face\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for each section of the PDFs\n",
    "doc_embeddings = model.encode(pdf_texts)\n",
    "3. Store Embeddings in a Vector Database\n",
    "Use Pinecone, FAISS, or Milvus to store and index the embeddings.\n",
    "\n",
    "a) Using Pinecone:\n",
    "python\n",
    "Copy code\n",
    "import pinecone\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key=\"your_pinecone_api_key\")\n",
    "index = pinecone.Index(\"pdf-documents\")\n",
    "\n",
    "# Prepare embeddings for upsert\n",
    "vector_data = [(f\"doc_{i}\", embedding) for i, embedding in enumerate(doc_embeddings)]\n",
    "\n",
    "# Upsert the vectors\n",
    "index.upsert(vectors=vector_data)\n",
    "b) Using FAISS:\n",
    "python\n",
    "Copy code\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Initialize FAISS index\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index_faiss = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Convert embeddings to numpy array and add to FAISS index\n",
    "doc_embeddings_np = np.array(doc_embeddings)\n",
    "index_faiss.add(doc_embeddings_np)\n",
    "c) Using Milvus:\n",
    "python\n",
    "Copy code\n",
    "from pymilvus import Collection, CollectionSchema, DataType\n",
    "\n",
    "# Define schema for Milvus\n",
    "schema = CollectionSchema(fields=[\n",
    "    {\"name\": \"embeddings\", \"type\": DataType.FLOAT_VECTOR, \"params\": {\"dim\": dimension}}\n",
    "])\n",
    "collection = Collection(name=\"pdf_documents\", schema=schema)\n",
    "\n",
    "# Insert embeddings into Milvus\n",
    "collection.insert([doc_embeddings])\n",
    "4. Query Processing and Similarity Search\n",
    "When a user submits a query, convert it to an embedding and search for similar documents.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Generate embedding for user query\n",
    "user_query = \"Find information about data privacy\"\n",
    "query_embedding = model.encode([user_query])[0]\n",
    "\n",
    "# Perform similarity search in Pinecone\n",
    "results_pinecone = index.query(queries=[query_embedding], top_k=5)\n",
    "\n",
    "# Perform similarity search in FAISS\n",
    "D, I = index_faiss.search(np.array([query_embedding]), k=5)\n",
    "\n",
    "# Perform similarity search in Milvus\n",
    "search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "results_milvus = collection.search([query_embedding], anns_field=\"embeddings\", param=search_params, limit=5)\n",
    "5. Retrieve and Display Results\n",
    "Fetch the most relevant sections or documents and display them to the user.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Example code for displaying results from Pinecone\n",
    "top_k_results = results_pinecone['matches']\n",
    "for match in top_k_results:\n",
    "    doc_id = match['id']\n",
    "    score = match['score']\n",
    "    print(f\"Document ID: {doc_id}, Similarity Score: {score}\")\n",
    "    # You can map the doc_id back to the actual document text or metadata\n",
    "python\n",
    "Copy code\n",
    "# Example code for displaying results from FAISS\n",
    "for idx in I[0]:\n",
    "    print(f\"Document Index: {idx}, Distance: {D[0][idx]}\")\n",
    "    # Map index to actual document text or metadata\n",
    "python\n",
    "Copy code\n",
    "# Example code for displaying results from Milvus\n",
    "for result in results_milvus:\n",
    "    for entity in result:\n",
    "        doc_id = entity.id\n",
    "        score = entity.distance\n",
    "        print(f\"Document ID: {doc_id}, Distance: {score}\")\n",
    "        # Map doc_id to actual document text or metadata\n",
    "Complete Code Example\n",
    "python\n",
    "Copy code\n",
    "import fitz  # PyMuPDF\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pinecone\n",
    "import faiss\n",
    "import numpy as np\n",
    "from pymilvus import Collection, CollectionSchema, DataType\n",
    "\n",
    "# Extract text from PDFs\n",
    "def extract_text_from_pdfs(pdf_paths):\n",
    "    texts = []\n",
    "    for path in pdf_paths:\n",
    "        pdf_document = fitz.open(path)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_document)):\n",
    "            page = pdf_document.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "# Generate embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "pdf_paths = [\"document1.pdf\", \"document2.pdf\"]\n",
    "pdf_texts = extract_text_from_pdfs(pdf_paths)\n",
    "doc_embeddings = model.encode(pdf_texts)\n",
    "\n",
    "# Store embeddings in Pinecone\n",
    "pinecone.init(api_key=\"your_pinecone_api_key\")\n",
    "index = pinecone.Index(\"pdf-documents\")\n",
    "vector_data = [(f\"doc_{i}\", embedding) for i, embedding in enumerate(doc_embeddings)]\n",
    "index.upsert(vectors=vector_data)\n",
    "\n",
    "# Store embeddings in FAISS\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index_faiss = faiss.IndexFlatL2(dimension)\n",
    "doc_embeddings_np = np.array(doc_embeddings)\n",
    "index_faiss.add(doc_embeddings_np)\n",
    "\n",
    "# Store embeddings in Milvus\n",
    "schema = CollectionSchema(fields=[\n",
    "    {\"name\": \"embeddings\", \"type\": DataType.FLOAT_VECTOR, \"params\": {\"dim\": dimension}}\n",
    "])\n",
    "collection = Collection(name=\"pdf_documents\", schema=schema)\n",
    "collection.insert([doc_embeddings])\n",
    "\n",
    "# Query processing and similarity search\n",
    "user_query = \"Find information about data privacy\"\n",
    "query_embedding = model.encode([user_query])[0]\n",
    "\n",
    "# Similarity search in Pinecone\n",
    "results_pinecone = index.query(queries=[query_embedding], top_k=5)\n",
    "\n",
    "# Similarity search in FAISS\n",
    "D, I = index_faiss.search(np.array([query_embedding]), k=5)\n",
    "\n",
    "# Similarity search in Milvus\n",
    "search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "results_milvus = collection.search([query_embedding], anns_field=\"embeddings\", param=search_params, limit=5)\n",
    "\n",
    "# Display results\n",
    "# Pinecone\n",
    "for match in results_pinecone['matches']:\n",
    "    doc_id = match['id']\n",
    "    score = match['score']\n",
    "    print(f\"Document ID: {doc_id}, Similarity Score: {score}\")\n",
    "\n",
    "# FAISS\n",
    "for idx in I[0]:\n",
    "    print(f\"Document Index: {idx}, Distance: {D[0][idx]}\")\n",
    "\n",
    "# Milvus\n",
    "for result in results_milvus:\n",
    "    for entity in result:\n",
    "        doc_id = entity.id\n",
    "        score = entity.distance\n",
    "        print(f\"Document ID: {doc_id}, Distance: {score}\")\n",
    "Summary\n",
    "This pipeline outlines the process of building a similarity search system using embeddings from Hugging Face Transformers and storing them in various vector databases. By integrating these components, you can create a robust search engine capable of understanding and retrieving relevant information across multiple PDF documents based on the semantic meaning of user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58bebd-6f7d-4cce-a002-674c12e50f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pinecone\n",
    "import faiss\n",
    "import numpy as np\n",
    "from pymilvus import Collection, CollectionSchema, DataType\n",
    "\n",
    "# Extract text from PDFs\n",
    "def extract_text_from_pdfs(pdf_paths):\n",
    "    texts = []\n",
    "    for path in pdf_paths:\n",
    "        pdf_document = fitz.open(path)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_document)):\n",
    "            page = pdf_document.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "# Generate embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "pdf_paths = [\"document1.pdf\", \"document2.pdf\"]\n",
    "pdf_texts = extract_text_from_pdfs(pdf_paths)\n",
    "doc_embeddings = model.encode(pdf_texts)\n",
    "\n",
    "# Store embeddings in Pinecone\n",
    "pinecone.init(api_key=\"your_pinecone_api_key\")\n",
    "index = pinecone.Index(\"pdf-documents\")\n",
    "vector_data = [(f\"doc_{i}\", embedding) for i, embedding in enumerate(doc_embeddings)]\n",
    "index.upsert(vectors=vector_data)\n",
    "\n",
    "# Store embeddings in FAISS\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index_faiss = faiss.IndexFlatL2(dimension)\n",
    "doc_embeddings_np = np.array(doc_embeddings)\n",
    "index_faiss.add(doc_embeddings_np)\n",
    "\n",
    "# Store embeddings in Milvus\n",
    "schema = CollectionSchema(fields=[\n",
    "    {\"name\": \"embeddings\", \"type\": DataType.FLOAT_VECTOR, \"params\": {\"dim\": dimension}}\n",
    "])\n",
    "collection = Collection(name=\"pdf_documents\", schema=schema)\n",
    "collection.insert([doc_embeddings])\n",
    "\n",
    "# Query processing and similarity search\n",
    "user_query = \"Find information about data privacy\"\n",
    "query_embedding = model.encode([user_query])[0]\n",
    "\n",
    "# Similarity search in Pinecone\n",
    "results_pinecone = index.query(queries=[query_embedding], top_k=5)\n",
    "\n",
    "# Similarity search in FAISS\n",
    "D, I = index_faiss.search(np.array([query_embedding]), k=5)\n",
    "\n",
    "# Similarity search in Milvus\n",
    "search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "results_milvus = collection.search([query_embedding], anns_field=\"embeddings\", param=search_params, limit=5)\n",
    "\n",
    "# Display results\n",
    "# Pinecone\n",
    "for match in results_pinecone['matches']:\n",
    "    doc_id = match['id']\n",
    "    score = match['score']\n",
    "    print(f\"Document ID: {doc_id}, Similarity Score: {score}\")\n",
    "\n",
    "# FAISS\n",
    "for idx in I[0]:\n",
    "    print(f\"Document Index: {idx}, Distance: {D[0][idx]}\")\n",
    "\n",
    "# Milvus\n",
    "for result in results_milvus:\n",
    "    for entity in result:\n",
    "        doc_id = entity.id\n",
    "        score = entity.distance\n",
    "        print(f\"Document ID: {doc_id}, Distance: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606756a-e29a-4f91-aa0c-7f4c690de6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b264cf-791a-4aeb-91c6-7752f60726fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To perform NLP similarity search using Hugging Face and compare multiple vector databases, you can follow these steps. I'll provide a detailed example that includes setting up a similarity search, performing searches, and comparing results from different vector databases.\n",
    "\n",
    "#Use Case\n",
    "#We have 4 dummy PDFs with lots of information, and we want to:\n",
    "\n",
    "Extract text from these PDFs.\n",
    "Convert the text into embeddings using a Hugging Face model.\n",
    "Store these embeddings in multiple vector databases.\n",
    "Perform similarity searches in each database.\n",
    "Compare the results from the different databases.\n",
    "Prerequisites\n",
    "Python installed\n",
    "Required libraries: PyMuPDF (for PDF extraction), transformers (for Hugging Face embeddings), faiss, pinecone, weaviate (for vector databases)\n",
    "Code Example\n",
    "1. Extract Text from PDFs\n",
    "python\n",
    "Copy code\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "pdf_paths = [\"pdf1.pdf\", \"pdf2.pdf\", \"pdf3.pdf\", \"pdf4.pdf\"]\n",
    "pdf_texts = [extract_text_from_pdf(pdf_path) for pdf_path in pdf_paths]\n",
    "2. Convert Text to Embeddings\n",
    "python\n",
    "Copy code\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "embeddings = [get_embedding(text) for text in pdf_texts]\n",
    "3. Store Embeddings in Vector Databases\n",
    "Faiss\n",
    "python\n",
    "Copy code\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "dimension = embeddings[0].shape[0]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "def add_to_faiss(embeddings):\n",
    "    index.add(np.array(embeddings))\n",
    "    return index\n",
    "\n",
    "faiss_index = add_to_faiss(embeddings)\n",
    "Pinecone\n",
    "python\n",
    "Copy code\n",
    "import pinecone\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key='your-pinecone-api-key', environment='us-west1-gcp')\n",
    "index_name = 'pdf-embeddings'\n",
    "pinecone.create_index(index_name, dimension=dimension)\n",
    "pinecone_index = pinecone.Index(index_name)\n",
    "\n",
    "def add_to_pinecone(embeddings):\n",
    "    pinecone_index.upsert(vectors=[(str(i), emb) for i, emb in enumerate(embeddings)])\n",
    "    return pinecone_index\n",
    "\n",
    "pinecone_index = add_to_pinecone(embeddings)\n",
    "Weaviate\n",
    "python\n",
    "Copy code\n",
    "import weaviate\n",
    "\n",
    "client = weaviate.Client(\"http://localhost:8080\")\n",
    "\n",
    "def create_weaviate_schema():\n",
    "    client.schema.create_class({\n",
    "        \"class\": \"Document\",\n",
    "        \"properties\": [\n",
    "            {\"name\": \"embedding\", \"dataType\": [\"blob\"]}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "def add_to_weaviate(embeddings):\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        client.data_object.create({\n",
    "            \"embedding\": emb\n",
    "        }, class_name=\"Document\")\n",
    "\n",
    "create_weaviate_schema()\n",
    "add_to_weaviate(embeddings)\n",
    "4. Perform Similarity Searches\n",
    "Faiss\n",
    "python\n",
    "Copy code\n",
    "def search_faiss(query_embedding, k=1):\n",
    "    distances, indices = faiss_index.search(np.array([query_embedding]), k)\n",
    "    return distances, indices\n",
    "\n",
    "query_text = \"example query\"\n",
    "query_embedding = get_embedding(query_text)\n",
    "distances, indices = search_faiss(query_embedding)\n",
    "print(\"Faiss Results:\", distances, indices)\n",
    "Pinecone\n",
    "python\n",
    "Copy code\n",
    "def search_pinecone(query_embedding, k=1):\n",
    "    result = pinecone_index.query(query_embedding, top_k=k)\n",
    "    return result\n",
    "\n",
    "query_embedding = get_embedding(query_text)\n",
    "result = search_pinecone(query_embedding)\n",
    "print(\"Pinecone Results:\", result)\n",
    "Weaviate\n",
    "python\n",
    "Copy code\n",
    "def search_weaviate(query_embedding, k=1):\n",
    "    result = client.query.get('Document', ['embedding']) \\\n",
    "        .with_near_vector({'vector': query_embedding}) \\\n",
    "        .with_limit(k) \\\n",
    "        .do()\n",
    "    return result\n",
    "\n",
    "query_embedding = get_embedding(query_text)\n",
    "result = search_weaviate(query_embedding)\n",
    "print(\"Weaviate Results:\", result)\n",
    "5. Compare Results\n",
    "python\n",
    "Copy code\n",
    "# Example comparison\n",
    "faiss_results = search_faiss(query_embedding)\n",
    "pinecone_results = search_pinecone(query_embedding)\n",
    "weaviate_results = search_weaviate(query_embedding)\n",
    "\n",
    "print(\"Faiss Results:\", faiss_results)\n",
    "print(\"Pinecone Results:\", pinecone_results)\n",
    "print(\"Weaviate Results:\", weaviate_results)\n",
    "Notes\n",
    "Faiss: A fast library for vector similarity search, but does not have built-in persistence. You need to manage saving and loading the index yourself.\n",
    "Pinecone: A managed vector database with built-in persistence and scaling.\n",
    "Weaviate: An open-source vector search engine with advanced features for schema management.\n",
    "Make sure to replace 'your-pinecone-api-key' with your actual Pinecone API key. Also, for Weaviate, ensure you have a running instance or use a hosted version.\n",
    "\n",
    "Feel free to adjust parameters and configurations based on your specific use case and needs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be44f623-98d0-4926-8c60-0f2ffc44bf2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d502f89e-ed05-4c20-ad44-775909618c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To perform NLP similarity search using Hugging Face and compare multiple vector databases, you can follow these steps. \n",
    "# I'll provide a detailed example that includes setting up a similarity search, \n",
    "# performing searches, and comparing results from different vector databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f411b0-7f97-4e81-99d8-7db46dc7f237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case\n",
    "# We have 4 dummy PDFs with lots of information, and we want to:\n",
    "\n",
    "# Extract text from these PDFs.\n",
    "# Convert the text into embeddings using a Hugging Face model.\n",
    "# Store these embeddings in multiple vector databases.\n",
    "# Perform similarity searches in each database.\n",
    "# Compare the results from the different databases.\n",
    "# Prerequisites\n",
    "# Python installed\n",
    "# Required libraries: PyMuPDF (for PDF extraction), transformers (for Hugging Face embeddings), faiss, pinecone, weaviate (for vector databases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aed8f065-4ead-4b84-b0dc-5cdc9509abf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fitz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#1. Extract Text from PDFs\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_text_from_pdf\u001b[39m(pdf_path):\n\u001b[0;32m      5\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fitz'"
     ]
    }
   ],
   "source": [
    "#1. Extract Text from PDFs\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "pdf_paths = [\"pdf1.pdf\", \"pdf2.pdf\", \"pdf3.pdf\", \"pdf4.pdf\"]\n",
    "pdf_texts = [extract_text_from_pdf(pdf_path) for pdf_path in pdf_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf5cd5-dc62-4ffd-9136-5231c4a8069a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f097759e-4088-4936-abe2-cbcb31b364de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04e3bd50-8ff1-409f-832b-40c2d0cb9cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"C:\\Users\\Himanshu Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#2. Convert Text to Embeddings\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     logging,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     52\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\__init__.py:34\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     add_code_sample_docstrings,\n\u001b[0;32m     28\u001b[0m     add_end_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     33\u001b[0m )\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     ContextManagers,\n\u001b[0;32m     36\u001b[0m     ExplicitEnum,\n\u001b[0;32m     37\u001b[0m     ModelOutput,\n\u001b[0;32m     38\u001b[0m     PaddingStrategy,\n\u001b[0;32m     39\u001b[0m     TensorType,\n\u001b[0;32m     40\u001b[0m     add_model_info_to_auto_map,\n\u001b[0;32m     41\u001b[0m     add_model_info_to_custom_pipelines,\n\u001b[0;32m     42\u001b[0m     cached_property,\n\u001b[0;32m     43\u001b[0m     can_return_loss,\n\u001b[0;32m     44\u001b[0m     expand_dims,\n\u001b[0;32m     45\u001b[0m     filter_out_non_signature_kwargs,\n\u001b[0;32m     46\u001b[0m     find_labels,\n\u001b[0;32m     47\u001b[0m     flatten_dict,\n\u001b[0;32m     48\u001b[0m     infer_framework,\n\u001b[0;32m     49\u001b[0m     is_jax_tensor,\n\u001b[0;32m     50\u001b[0m     is_numpy_array,\n\u001b[0;32m     51\u001b[0m     is_tensor,\n\u001b[0;32m     52\u001b[0m     is_tf_symbolic_tensor,\n\u001b[0;32m     53\u001b[0m     is_tf_tensor,\n\u001b[0;32m     54\u001b[0m     is_torch_device,\n\u001b[0;32m     55\u001b[0m     is_torch_dtype,\n\u001b[0;32m     56\u001b[0m     is_torch_tensor,\n\u001b[0;32m     57\u001b[0m     reshape,\n\u001b[0;32m     58\u001b[0m     squeeze,\n\u001b[0;32m     59\u001b[0m     strtobool,\n\u001b[0;32m     60\u001b[0m     tensor_size,\n\u001b[0;32m     61\u001b[0m     to_numpy,\n\u001b[0;32m     62\u001b[0m     to_py_obj,\n\u001b[0;32m     63\u001b[0m     torch_float,\n\u001b[0;32m     64\u001b[0m     torch_int,\n\u001b[0;32m     65\u001b[0m     transpose,\n\u001b[0;32m     66\u001b[0m     working_or_temp_dir,\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     69\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[0;32m     70\u001b[0m     HF_MODULES_CACHE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     96\u001b[0m     try_to_load_from_cache,\n\u001b[0;32m     97\u001b[0m )\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     99\u001b[0m     ACCELERATE_MIN_VERSION,\n\u001b[0;32m    100\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m     torch_only_method,\n\u001b[0;32m    220\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:462\u001b[0m\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 462\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytree\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_torch_pytree\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_model_output_flatten\u001b[39m(output: ModelOutput) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[Any], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_pytree.Context\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39mvalues()), \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\__init__.py:148\u001b[0m\n\u001b[0;32m    146\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    147\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 148\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    150\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"C:\\Users\\Himanshu Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "#2. Convert Text to Embeddings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "embeddings = [get_embedding(text) for text in pdf_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a26411-2f23-4140-abeb-f31876305c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Store Embeddings in Vector Databases\n",
    "# Faiss\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "dimension = embeddings[0].shape[0]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "def add_to_faiss(embeddings):\n",
    "    index.add(np.array(embeddings))\n",
    "    return index\n",
    "\n",
    "faiss_index = add_to_faiss(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec5902c-4706-4480-a81e-be5931568f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pinecone\n",
    "import pinecone\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key='your-pinecone-api-key', environment='us-west1-gcp')\n",
    "index_name = 'pdf-embeddings'\n",
    "pinecone.create_index(index_name, dimension=dimension)\n",
    "pinecone_index = pinecone.Index(index_name)\n",
    "\n",
    "def add_to_pinecone(embeddings):\n",
    "    pinecone_index.upsert(vectors=[(str(i), emb) for i, emb in enumerate(embeddings)])\n",
    "    return pinecone_index\n",
    "\n",
    "pinecone_index = add_to_pinecone(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a7889e-a622-4be0-b578-b4d10af59b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weaviate\n",
    "import weaviate\n",
    "\n",
    "client = weaviate.Client(\"http://localhost:8080\")\n",
    "\n",
    "def create_weaviate_schema():\n",
    "    client.schema.create_class({\n",
    "        \"class\": \"Document\",\n",
    "        \"properties\": [\n",
    "            {\"name\": \"embedding\", \"dataType\": [\"blob\"]}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "def add_to_weaviate(embeddings):\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        client.data_object.create({\n",
    "            \"embedding\": emb\n",
    "        }, class_name=\"Document\")\n",
    "\n",
    "create_weaviate_schema()\n",
    "add_to_weaviate(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e0dda-f552-42f2-a031-2828c1cbd219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Perform Similarity Searches\n",
    "# Faiss\n",
    "def search_faiss(query_embedding, k=1):\n",
    "    distances, indices = faiss_index.search(np.array([query_embedding]), k)\n",
    "    return distances, indices\n",
    "\n",
    "query_text = \"example query\"\n",
    "query_embedding = get_embedding(query_text)\n",
    "distances, indices = search_faiss(query_embedding)\n",
    "print(\"Faiss Results:\", distances, indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf4889f-ed91-43b1-9b35-b10c7ad3ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pinecone\n",
    "def search_pinecone(query_embedding, k=1):\n",
    "    result = pinecone_index.query(query_embedding, top_k=k)\n",
    "    return result\n",
    "\n",
    "query_embedding = get_embedding(query_text)\n",
    "result = search_pinecone(query_embedding)\n",
    "print(\"Pinecone Results:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03ec5eb-f62c-4c3b-9419-bce91cb5e07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weaviate\n",
    "def search_weaviate(query_embedding, k=1):\n",
    "    result = client.query.get('Document', ['embedding']) \\\n",
    "        .with_near_vector({'vector': query_embedding}) \\\n",
    "        .with_limit(k) \\\n",
    "        .do()\n",
    "    return result\n",
    "\n",
    "query_embedding = get_embedding(query_text)\n",
    "result = search_weaviate(query_embedding)\n",
    "print(\"Weaviate Results:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b7c131-5eca-4189-af69-60c831f7f114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Compare Results\n",
    "# Example comparison\n",
    "faiss_results = search_faiss(query_embedding)\n",
    "pinecone_results = search_pinecone(query_embedding)\n",
    "weaviate_results = search_weaviate(query_embedding)\n",
    "\n",
    "print(\"Faiss Results:\", faiss_results)\n",
    "print(\"Pinecone Results:\", pinecone_results)\n",
    "print(\"Weaviate Results:\", weaviate_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a9b01b-b238-40b0-9acc-3bf5e42d1397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes\n",
    "# Faiss: A fast library for vector similarity search, but does not have built-in persistence. You need to manage saving and loading the index yourself.\n",
    "# Pinecone: A managed vector database with built-in persistence and scaling.\n",
    "# Weaviate: An open-source vector search engine with advanced features for schema management.\n",
    "# Make sure to replace 'your-pinecone-api-key' with your actual Pinecone API key. Also, for Weaviate, ensure you have a running instance or use a hosted version.\n",
    "\n",
    "# Feel free to adjust parameters and configurations based on your specific use case and needs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465d4947-559f-48f0-be6c-4021b5ccd568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9603c524-decf-4976-862e-ce69f51658f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Absolutely, vector databases can significantly enhance the speed and effectiveness of product recommendation systems in e-commerce. Here’s how you can leverage vector databases for product recommendations:\n",
    "\n",
    "Use Case: E-Commerce Product Recommendations\n",
    "Scenario:\n",
    "An e-commerce site aims to provide personalized product recommendations based on customer browsing and purchase history. The goal is to identify and recommend products that customers are likely to be interested in, often in near real-time.\n",
    "\n",
    "Steps for Implementation:\n",
    "Data Collection:\n",
    "\n",
    "User Data: Collect data on user interactions such as browsing history, clicks, purchases, and ratings.\n",
    "Product Data: Gather information on products including descriptions, categories, prices, and features.\n",
    "Feature Engineering:\n",
    "\n",
    "User Embeddings: Create embeddings for users based on their interaction history.\n",
    "Product Embeddings: Generate embeddings for products using their features and descriptions.\n",
    "Convert Data into Vectors:\n",
    "\n",
    "User Vectors: Create vectors representing user preferences.\n",
    "Product Vectors: Convert product data into vectors using techniques like:\n",
    "TF-IDF + PCA: For text-based product descriptions.\n",
    "Deep Learning Models: Use pre-trained models to generate embeddings for product features.\n",
    "Store Vectors in a Vector Database:\n",
    "\n",
    "Choose a Vector Database: Select a vector database that fits your needs (e.g., FAISS, Pinecone, Weaviate).\n",
    "Perform Similarity Searches:\n",
    "\n",
    "Query with User Vectors: Use user vectors to perform similarity searches in the vector database to find and recommend products that are similar to those the user has shown interest in.\n",
    "Deliver Recommendations:\n",
    "\n",
    "Top-K Recommendations: Provide the top-K most similar products as recommendations to the user.\n",
    "Detailed Code Example\n",
    "1. Data Collection\n",
    "Assume you have a dataset of user interactions and product details in CSV files.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "user_data = pd.read_csv('user_interactions.csv')  # Contains user_id, product_id, interaction_type\n",
    "product_data = pd.read_csv('products.csv')  # Contains product_id, description, category\n",
    "2. Feature Engineering\n",
    "Generate embeddings for users and products.\n",
    "\n",
    "Product Embeddings\n",
    "python\n",
    "Copy code\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Generate product embeddings\n",
    "product_embeddings = product_data['description'].apply(get_embedding).tolist()\n",
    "User Embeddings\n",
    "python\n",
    "Copy code\n",
    "# Aggregate user interactions to create a user profile vector\n",
    "user_profiles = user_data.groupby('user_id')['product_id'].apply(list)\n",
    "\n",
    "def create_user_vector(user_products):\n",
    "    # Aggregate product vectors for a user\n",
    "    product_vecs = [product_embeddings[product_data.index[product_data['product_id'] == pid].tolist()[0]] for pid in user_products]\n",
    "    return np.mean(product_vecs, axis=0)\n",
    "\n",
    "user_vectors = user_profiles.apply(create_user_vector).tolist()\n",
    "3. Store Vectors in Vector Databases\n",
    "FAISS\n",
    "python\n",
    "Copy code\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "dimension = len(product_embeddings[0])\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(product_embeddings))\n",
    "\n",
    "# Save the index to disk\n",
    "faiss.write_index(index, 'faiss_product_index.index')\n",
    "Pinecone\n",
    "python\n",
    "Copy code\n",
    "import pinecone\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key='your-pinecone-api-key', environment='us-west1-gcp')\n",
    "index_name = 'product-recommendations'\n",
    "pinecone.create_index(index_name, dimension=dimension)\n",
    "pinecone_index = pinecone.Index(index_name)\n",
    "\n",
    "# Upsert product vectors into Pinecone\n",
    "pinecone_index.upsert(vectors=[(str(i), vec) for i, vec in enumerate(product_embeddings)])\n",
    "Weaviate\n",
    "python\n",
    "Copy code\n",
    "import weaviate\n",
    "\n",
    "client = weaviate.Client(\"http://localhost:8080\")\n",
    "\n",
    "# Create Weaviate schema\n",
    "client.schema.create_class({\n",
    "    \"class\": \"Product\",\n",
    "    \"properties\": [\n",
    "        {\"name\": \"vector\", \"dataType\": [\"blob\"]}\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Add product vectors to Weaviate\n",
    "for vec in product_embeddings:\n",
    "    client.data_object.create({\n",
    "        \"vector\": vec.tolist()\n",
    "    }, class_name=\"Product\")\n",
    "4. Perform Similarity Searches\n",
    "FAISS\n",
    "python\n",
    "Copy code\n",
    "def search_faiss(user_vector, k=5):\n",
    "    distances, indices = index.search(np.array([user_vector]), k)\n",
    "    return distances, indices\n",
    "\n",
    "# Example user query\n",
    "query_vector = create_user_vector(['product1_id', 'product2_id'])\n",
    "distances, indices = search_faiss(query_vector)\n",
    "print(\"Faiss Recommendations:\", indices)\n",
    "Pinecone\n",
    "python\n",
    "Copy code\n",
    "def search_pinecone(user_vector, k=5):\n",
    "    result = pinecone_index.query(user_vector, top_k=k)\n",
    "    return result\n",
    "\n",
    "query_vector = create_user_vector(['product1_id', 'product2_id'])\n",
    "result = search_pinecone(query_vector)\n",
    "print(\"Pinecone Recommendations:\", result)\n",
    "Weaviate\n",
    "python\n",
    "Copy code\n",
    "def search_weaviate(user_vector, k=5):\n",
    "    result = client.query.get('Product', ['vector']) \\\n",
    "        .with_near_vector({'vector': user_vector.tolist()}) \\\n",
    "        .with_limit(k) \\\n",
    "        .do()\n",
    "    return result\n",
    "\n",
    "query_vector = create_user_vector(['product1_id', 'product2_id'])\n",
    "result = search_weaviate(query_vector)\n",
    "print(\"Weaviate Recommendations:\", result)\n",
    "5. Deliver Recommendations\n",
    "Use the search results to display personalized product recommendations to users in real-time.\n",
    "\n",
    "Summary\n",
    "By leveraging vector databases, you can create a highly responsive and personalized recommendation system. The use of embeddings allows for nuanced comparisons between products and user preferences, providing recommendations that are relevant and engaging. The choice of vector database (FAISS, Pinecone, Weaviate) will depend on your scalability needs and specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617fa8fe-ec30-4a8e-b9f3-d14d9572dcdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d1dd0c-7064-4324-989e-f697ced4f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Certainly! To enhance e-commerce product recommendations using vector databases and Hugging Face, you can leverage the following approach. This method involves extracting embeddings from customer and product data, storing these embeddings in a vector database, and performing fast similarity searches to deliver personalized recommendations in near real-time.\n",
    "\n",
    "Steps for Implementation\n",
    "Data Collection and Preprocessing\n",
    "Feature Extraction with Hugging Face\n",
    "Storing Embeddings in Vector Databases\n",
    "Performing Similarity Searches\n",
    "Delivering Recommendations\n",
    "1. Data Collection and Preprocessing\n",
    "Assume you have data about customer interactions with products (e.g., browsing history, purchase history) and product details (e.g., descriptions, features).\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Load customer interaction data and product data\n",
    "customer_data = pd.read_csv('customer_interactions.csv')  # customer_id, product_id, interaction_type\n",
    "product_data = pd.read_csv('products.csv')  # product_id, description, category\n",
    "2. Feature Extraction with Hugging Face\n",
    "Use Hugging Face’s pre-trained models to generate embeddings for product descriptions and customer interactions.\n",
    "\n",
    "Product Embeddings\n",
    "python\n",
    "Copy code\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Generate embeddings for product descriptions\n",
    "product_embeddings = product_data['description'].apply(get_embedding).tolist()\n",
    "Customer Embeddings\n",
    "Aggregate customer interactions to create a profile vector.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Aggregate product embeddings for each customer\n",
    "user_profiles = customer_data.groupby('customer_id')['product_id'].apply(list)\n",
    "\n",
    "def create_user_vector(user_products):\n",
    "    product_vecs = [product_embeddings[product_data.index[product_data['product_id'] == pid].tolist()[0]] for pid in user_products]\n",
    "    return np.mean(product_vecs, axis=0)\n",
    "\n",
    "# Generate embeddings for customer profiles\n",
    "user_vectors = user_profiles.apply(create_user_vector).tolist()\n",
    "3. Storing Embeddings in Vector Databases\n",
    "Store product embeddings and customer profile embeddings in a vector database for efficient similarity searches.\n",
    "\n",
    "FAISS\n",
    "python\n",
    "Copy code\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "dimension = len(product_embeddings[0])\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(product_embeddings))\n",
    "\n",
    "# Save the index to disk\n",
    "faiss.write_index(index, 'faiss_product_index.index')\n",
    "Pinecone\n",
    "python\n",
    "Copy code\n",
    "import pinecone\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key='your-pinecone-api-key', environment='us-west1-gcp')\n",
    "index_name = 'product-recommendations'\n",
    "pinecone.create_index(index_name, dimension=dimension)\n",
    "pinecone_index = pinecone.Index(index_name)\n",
    "\n",
    "# Upsert product vectors into Pinecone\n",
    "pinecone_index.upsert(vectors=[(str(i), vec) for i, vec in enumerate(product_embeddings)])\n",
    "Weaviate\n",
    "python\n",
    "Copy code\n",
    "import weaviate\n",
    "\n",
    "client = weaviate.Client(\"http://localhost:8080\")\n",
    "\n",
    "# Create Weaviate schema\n",
    "client.schema.create_class({\n",
    "    \"class\": \"Product\",\n",
    "    \"properties\": [\n",
    "        {\"name\": \"vector\", \"dataType\": [\"blob\"]}\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Add product vectors to Weaviate\n",
    "for vec in product_embeddings:\n",
    "    client.data_object.create({\n",
    "        \"vector\": vec.tolist()\n",
    "    }, class_name=\"Product\")\n",
    "4. Performing Similarity Searches\n",
    "Query the vector database to find similar products based on user profile vectors.\n",
    "\n",
    "FAISS\n",
    "python\n",
    "Copy code\n",
    "def search_faiss(user_vector, k=5):\n",
    "    distances, indices = index.search(np.array([user_vector]), k)\n",
    "    return distances, indices\n",
    "\n",
    "# Example query\n",
    "query_vector = create_user_vector(['product1_id', 'product2_id'])\n",
    "distances, indices = search_faiss(query_vector)\n",
    "print(\"Faiss Recommendations:\", indices)\n",
    "Pinecone\n",
    "python\n",
    "Copy code\n",
    "def search_pinecone(user_vector, k=5):\n",
    "    result = pinecone_index.query(user_vector, top_k=k)\n",
    "    return result\n",
    "\n",
    "query_vector = create_user_vector(['product1_id', 'product2_id'])\n",
    "result = search_pinecone(query_vector)\n",
    "print(\"Pinecone Recommendations:\", result)\n",
    "Weaviate\n",
    "python\n",
    "Copy code\n",
    "def search_weaviate(user_vector, k=5):\n",
    "    result = client.query.get('Product', ['vector']) \\\n",
    "        .with_near_vector({'vector': user_vector.tolist()}) \\\n",
    "        .with_limit(k) \\\n",
    "        .do()\n",
    "    return result\n",
    "\n",
    "query_vector = create_user_vector(['product1_id', 'product2_id'])\n",
    "result = search_weaviate(query_vector)\n",
    "print(\"Weaviate Recommendations:\", result)\n",
    "5. Delivering Recommendations\n",
    "Integrate the search results into your e-commerce platform to display personalized recommendations to users.\n",
    "\n",
    "Summary\n",
    "By using Hugging Face models to create embeddings and vector databases to perform similarity searches, you can significantly enhance the speed and accuracy of product recommendations on e-commerce sites. The system provides recommendations that are highly relevant to user interests, giving the impression of a personalized experience, often perceived as almost intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93836c8d-20f6-448e-9e05-e82f674126e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eef9e2-6f32-45b2-83b2-967848bb0a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "Absolutely, using Hugging Face models and vector databases can transform the way e-commerce sites handle product recommendations. Here’s a detailed approach to implementing a fast, scalable recommendation system using these technologies:\n",
    "\n",
    "Enhanced E-Commerce Product Recommendations\n",
    "1. Overview\n",
    "E-commerce sites aim to provide real-time, personalized recommendations to users based on their browsing and purchase history. Traditional data mining methods can be slow and less responsive, especially as user behavior changes rapidly. Vector databases and Hugging Face models can accelerate this process by enabling faster, more accurate similarity searches.\n",
    "\n",
    "2. Key Components\n",
    "Feature Extraction with Hugging Face\n",
    "Storing and Managing Vectors in Vector Databases\n",
    "Real-Time Similarity Searches\n",
    "Delivering Personalized Recommendations\n",
    "3. Feature Extraction with Hugging Face\n",
    "Objective: Generate embeddings for products and user interactions to capture their semantic meaning.\n",
    "\n",
    "Product Embeddings\n",
    "Load and Preprocess Data:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load product data\n",
    "product_data = pd.read_csv('products.csv')  # product_id, description, category\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Generate embeddings for product descriptions\n",
    "product_data['embedding'] = product_data['description'].apply(get_embedding)\n",
    "User Embeddings\n",
    "Aggregate User Interactions:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "user_data = pd.read_csv('customer_interactions.csv')  # customer_id, product_id, interaction_type\n",
    "user_profiles = user_data.groupby('customer_id')['product_id'].apply(list)\n",
    "\n",
    "def create_user_vector(user_products):\n",
    "    product_vecs = [product_data.loc[product_data['product_id'] == pid, 'embedding'].values[0] for pid in user_products]\n",
    "    return np.mean(product_vecs, axis=0)\n",
    "\n",
    "# Generate embeddings for user profiles\n",
    "user_data['embedding'] = user_profiles.apply(create_user_vector)\n",
    "4. Storing and Managing Vectors in Vector Databases\n",
    "Objective: Efficiently store and query high-dimensional vectors representing products and user profiles.\n",
    "\n",
    "Using FAISS\n",
    "Store Product Embeddings:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "dimension = len(product_data['embedding'].iloc[0])\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(product_data['embedding'].tolist()))\n",
    "\n",
    "# Save the index to disk\n",
    "faiss.write_index(index, 'faiss_product_index.index')\n",
    "Using Pinecone\n",
    "Store Vectors in Pinecone:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pinecone\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key='your-pinecone-api-key', environment='us-west1-gcp')\n",
    "index_name = 'product-recommendations'\n",
    "pinecone.create_index(index_name, dimension=dimension)\n",
    "pinecone_index = pinecone.Index(index_name)\n",
    "\n",
    "# Upsert product vectors\n",
    "pinecone_index.upsert(vectors=[(str(i), vec) for i, vec in enumerate(product_data['embedding'].tolist())])\n",
    "Using Weaviate\n",
    "Store Vectors in Weaviate:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import weaviate\n",
    "\n",
    "client = weaviate.Client(\"http://localhost:8080\")\n",
    "\n",
    "# Create Weaviate schema\n",
    "client.schema.create_class({\n",
    "    \"class\": \"Product\",\n",
    "    \"properties\": [\n",
    "        {\"name\": \"vector\", \"dataType\": [\"blob\"]}\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Add product vectors\n",
    "for vec in product_data['embedding']:\n",
    "    client.data_object.create({\"vector\": vec.tolist()}, class_name=\"Product\")\n",
    "5. Real-Time Similarity Searches\n",
    "Objective: Quickly find products similar to a user’s profile or current behavior.\n",
    "\n",
    "Using FAISS\n",
    "Perform Search:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def search_faiss(user_vector, k=5):\n",
    "    distances, indices = index.search(np.array([user_vector]), k)\n",
    "    return distances, indices\n",
    "\n",
    "# Example query\n",
    "query_vector = create_user_vector(['product1_id', 'product2_id'])\n",
    "distances, indices = search_faiss(query_vector)\n",
    "print(\"Faiss Recommendations:\", indices)\n",
    "Using Pinecone\n",
    "Perform Search:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def search_pinecone(user_vector, k=5):\n",
    "    result = pinecone_index.query(user_vector, top_k=k)\n",
    "    return result\n",
    "\n",
    "query_vector = create_user_vector(['product1_id', 'product2_id'])\n",
    "result = search_pinecone(query_vector)\n",
    "print(\"Pinecone Recommendations:\", result)\n",
    "Using Weaviate\n",
    "Perform Search:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def search_weaviate(user_vector, k=5):\n",
    "    result = client.query.get('Product', ['vector']) \\\n",
    "        .with_near_vector({'vector': user_vector.tolist()}) \\\n",
    "        .with_limit(k) \\\n",
    "        .do()\n",
    "    return result\n",
    "\n",
    "query_vector = create_user_vector(['product1_id', 'product2_id'])\n",
    "result = search_weaviate(query_vector)\n",
    "print(\"Weaviate Recommendations:\", result)\n",
    "6. Delivering Personalized Recommendations\n",
    "Integrate the search results into your e-commerce platform to provide users with relevant, personalized product recommendations. This integration can include:\n",
    "\n",
    "Real-Time Display: Show recommendations as users browse or interact with the site.\n",
    "Personalized Alerts: Notify users of new or recommended products based on their interests.\n",
    "Dynamic Updates: Continuously update recommendations based on user interactions and new product data.\n",
    "Summary\n",
    "By combining Hugging Face’s advanced NLP models with vector databases, e-commerce sites can provide highly personalized, real-time product recommendations. The system identifies patterns in user behavior and product data with remarkable speed and accuracy, offering an engaging experience that feels intuitively aligned with user preferences.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba114031-973c-4357-a3f2-af7653529447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f045c2d-a409-401b-ad6a-10f362d5420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Leveraging LangChain, OpenAI, and vector databases for e-commerce product recommendations can greatly enhance the speed and relevance of the recommendations provided. Here’s a detailed approach to implement such a system:\n",
    "\n",
    "Enhanced E-Commerce Product Recommendations\n",
    "1. Overview\n",
    "E-commerce platforms need to provide real-time, personalized recommendations. Traditional data mining approaches can be too slow for this purpose. By integrating LangChain with OpenAI’s powerful language models and vector databases, you can deliver recommendations faster and more effectively, creating a highly engaging experience for users.\n",
    "\n",
    "2. Key Components\n",
    "Feature Extraction and Embedding with OpenAI\n",
    "Vector Storage and Management\n",
    "Real-Time Similarity Searches\n",
    "LangChain Integration for Enhanced Interaction\n",
    "Delivering Personalized Recommendations\n",
    "3. Feature Extraction and Embedding with OpenAI\n",
    "Objective: Generate embeddings for products and user interactions to capture their semantic meaning.\n",
    "\n",
    "Product Embeddings\n",
    "Load and Preprocess Data:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "# Load product data\n",
    "product_data = pd.read_csv('products.csv')  # product_id, description, category\n",
    "\n",
    "# OpenAI API setup\n",
    "openai.api_key = 'your-openai-api-key'\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = openai.Embedding.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response['data'][0]['embedding']\n",
    "\n",
    "# Generate embeddings for product descriptions\n",
    "product_data['embedding'] = product_data['description'].apply(get_embedding)\n",
    "User Embeddings\n",
    "Aggregate User Interactions:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "user_data = pd.read_csv('customer_interactions.csv')  # customer_id, product_id, interaction_type\n",
    "user_profiles = user_data.groupby('customer_id')['product_id'].apply(list)\n",
    "\n",
    "def create_user_vector(user_products):\n",
    "    product_vecs = [product_data.loc[product_data['product_id'] == pid, 'embedding'].values[0] for pid in user_products]\n",
    "    return np.mean(product_vecs, axis=0)\n",
    "\n",
    "# Generate embeddings for user profiles\n",
    "user_profiles = user_profiles.apply(create_user_vector)\n",
    "4. Vector Storage and Management\n",
    "Objective: Efficiently store and query high-dimensional vectors representing products and user profiles.\n",
    "\n",
    "Using FAISS\n",
    "Store Product Embeddings:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "dimension = len(product_data['embedding'].iloc[0])\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(product_data['embedding'].tolist()))\n",
    "\n",
    "# Save the index to disk\n",
    "faiss.write_index(index, 'faiss_product_index.index')\n",
    "Using Pinecone\n",
    "Store Vectors in Pinecone:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pinecone\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key='your-pinecone-api-key', environment='us-west1-gcp')\n",
    "index_name = 'product-recommendations'\n",
    "pinecone.create_index(index_name, dimension=dimension)\n",
    "pinecone_index = pinecone.Index(index_name)\n",
    "\n",
    "# Upsert product vectors\n",
    "pinecone_index.upsert(vectors=[(str(i), vec) for i, vec in enumerate(product_data['embedding'].tolist())])\n",
    "Using Weaviate\n",
    "Store Vectors in Weaviate:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import weaviate\n",
    "\n",
    "client = weaviate.Client(\"http://localhost:8080\")\n",
    "\n",
    "# Create Weaviate schema\n",
    "client.schema.create_class({\n",
    "    \"class\": \"Product\",\n",
    "    \"properties\": [\n",
    "        {\"name\": \"vector\", \"dataType\": [\"blob\"]}\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Add product vectors\n",
    "for vec in product_data['embedding']:\n",
    "    client.data_object.create({\"vector\": vec.tolist()}, class_name=\"Product\")\n",
    "5. Real-Time Similarity Searches\n",
    "Objective: Quickly find similar products based on user profile vectors.\n",
    "\n",
    "Using FAISS\n",
    "Perform Search:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def search_faiss(user_vector, k=5):\n",
    "    distances, indices = index.search(np.array([user_vector]), k)\n",
    "    return distances, indices\n",
    "\n",
    "query_vector = create_user_vector(['product1_id', 'product2_id'])\n",
    "distances, indices = search_faiss(query_vector)\n",
    "print(\"Faiss Recommendations:\", indices)\n",
    "Using Pinecone\n",
    "Perform Search:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def search_pinecone(user_vector, k=5):\n",
    "    result = pinecone_index.query(user_vector, top_k=k)\n",
    "    return result\n",
    "\n",
    "query_vector = create_user_vector(['product1_id', 'product2_id'])\n",
    "result = search_pinecone(query_vector)\n",
    "print(\"Pinecone Recommendations:\", result)\n",
    "Using Weaviate\n",
    "Perform Search:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def search_weaviate(user_vector, k=5):\n",
    "    result = client.query.get('Product', ['vector']) \\\n",
    "        .with_near_vector({'vector': user_vector.tolist()}) \\\n",
    "        .with_limit(k) \\\n",
    "        .do()\n",
    "    return result\n",
    "\n",
    "query_vector = create_user_vector(['product1_id', 'product2_id'])\n",
    "result = search_weaviate(query_vector)\n",
    "print(\"Weaviate Recommendations:\", result)\n",
    "6. Integrating LangChain for Enhanced Interaction\n",
    "LangChain can be utilized to build interactive and conversational elements that enhance user experience.\n",
    "\n",
    "Setup LangChain:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from langchain import OpenAI, Chain\n",
    "\n",
    "# Initialize LangChain with OpenAI GPT model\n",
    "openai_chain = OpenAI(api_key='your-openai-api-key')\n",
    "\n",
    "# Define a chain for generating recommendations\n",
    "class RecommendationChain(Chain):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(model)\n",
    "        self.model = model\n",
    "\n",
    "    def run(self, query):\n",
    "        # Generate recommendations based on user input\n",
    "        response = self.model.generate(query)\n",
    "        return response\n",
    "\n",
    "# Create a recommendation chain instance\n",
    "recommendation_chain = RecommendationChain(openai_chain)\n",
    "Generate Recommendations:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "query = \"Recommend products similar to those liked by user XYZ.\"\n",
    "recommendations = recommendation_chain.run(query)\n",
    "print(\"LangChain Recommendations:\", recommendations)\n",
    "7. Delivering Personalized Recommendations\n",
    "Integrate the recommendations into your e-commerce platform to provide users with timely, relevant suggestions. This can be done through:\n",
    "\n",
    "Real-Time Display: Show recommendations as users interact with the site.\n",
    "Personalized Alerts: Notify users of new or related products based on their interests.\n",
    "Dynamic Updates: Continuously refine recommendations based on user interactions and new product data.\n",
    "Summary\n",
    "Combining LangChain, OpenAI’s embeddings, and vector databases allows e-commerce platforms to deliver highly personalized recommendations quickly. The system will leverage advanced NLP to understand and process user preferences, while vector databases enable rapid similarity searches, enhancing the overall user experience with insightful and timely recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d664f7cf-8e13-4490-aa47-f5f984d02e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e1424-deb0-4b44-b046-8726f2f8e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "To build a recommendation system for Netflix movies using Hugging Face NLP and vector databases, follow these steps:\n",
    "\n",
    "Recommendation System for Netflix Movies Using Hugging Face and Vector Databases\n",
    "1. Overview\n",
    "By using Hugging Face’s NLP models for generating embeddings and a vector database for storage and similarity search, you can build an effective recommendation system that provides personalized movie suggestions based on user preferences and movie similarities.\n",
    "\n",
    "2. Key Components\n",
    "Data Preparation\n",
    "Feature Extraction and Embedding with Hugging Face\n",
    "Vector Storage and Management\n",
    "Real-Time Similarity Searches\n",
    "Recommendation Generation\n",
    "3. Data Preparation\n",
    "Objective: Collect and preprocess movie and user interaction data.\n",
    "\n",
    "Load Movie Data:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Load movie dataset\n",
    "movies_df = pd.read_csv('movies.csv')  # columns: movie_id, title, description, genre\n",
    "Load User Interaction Data:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Load user interaction data\n",
    "interactions_df = pd.read_csv('user_interactions.csv')  # columns: user_id, movie_id, rating\n",
    "4. Feature Extraction and Embedding with Hugging Face\n",
    "Objective: Convert movie descriptions and user profiles into vector embeddings using Hugging Face models.\n",
    "\n",
    "Setup Hugging Face Transformers:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "Generate Movie Embeddings:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
    "\n",
    "movies_df['embedding'] = movies_df['description'].apply(get_embedding)\n",
    "Generate User Profile Embeddings:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "\n",
    "# Aggregate user interactions to create a user profile vector\n",
    "user_profiles = interactions_df.groupby('user_id')['movie_id'].apply(list)\n",
    "\n",
    "def create_user_vector(user_movies):\n",
    "    movie_vecs = [movies_df.loc[movies_df['movie_id'] == mid, 'embedding'].values[0] for mid in user_movies]\n",
    "    return np.mean(movie_vecs, axis=0)\n",
    "\n",
    "user_profiles = user_profiles.apply(create_user_vector)\n",
    "5. Vector Storage and Management\n",
    "Objective: Store and manage vectors efficiently using a vector database.\n",
    "\n",
    "Using FAISS:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "dimension = len(movies_df['embedding'].iloc[0])\n",
    "movie_index = faiss.IndexFlatL2(dimension)\n",
    "movie_index.add(np.array(movies_df['embedding'].tolist()))\n",
    "\n",
    "# Save the index to disk\n",
    "faiss.write_index(movie_index, 'faiss_movie_index.index')\n",
    "Using Pinecone:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pinecone\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key='your-pinecone-api-key', environment='us-west1-gcp')\n",
    "index_name = 'movie-recommendations'\n",
    "pinecone.create_index(index_name, dimension=dimension)\n",
    "pinecone_index = pinecone.Index(index_name)\n",
    "\n",
    "# Upsert movie vectors\n",
    "pinecone_index.upsert(vectors=[(str(i), vec) for i, vec in enumerate(movies_df['embedding'].tolist())])\n",
    "Using Weaviate:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import weaviate\n",
    "\n",
    "client = weaviate.Client(\"http://localhost:8080\")\n",
    "\n",
    "# Create Weaviate schema\n",
    "client.schema.create_class({\n",
    "    \"class\": \"Movie\",\n",
    "    \"properties\": [\n",
    "        {\"name\": \"vector\", \"dataType\": [\"blob\"]}\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Add movie vectors\n",
    "for vec in movies_df['embedding']:\n",
    "    client.data_object.create({\"vector\": vec.tolist()}, class_name=\"Movie\")\n",
    "6. Real-Time Similarity Searches\n",
    "Objective: Perform similarity searches to recommend movies based on user profile vectors.\n",
    "\n",
    "Using FAISS:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def recommend_movies_faiss(user_vector, k=5):\n",
    "    distances, indices = movie_index.search(np.array([user_vector]), k)\n",
    "    return indices\n",
    "\n",
    "query_vector = create_user_vector(['movie1_id', 'movie2_id'])\n",
    "recommended_movie_indices = recommend_movies_faiss(query_vector)\n",
    "print(\"Faiss Recommendations:\", recommended_movie_indices)\n",
    "Using Pinecone:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def recommend_movies_pinecone(user_vector, k=5):\n",
    "    result = pinecone_index.query(user_vector, top_k=k)\n",
    "    return result\n",
    "\n",
    "query_vector = create_user_vector(['movie1_id', 'movie2_id'])\n",
    "recommended_movies = recommend_movies_pinecone(query_vector)\n",
    "print(\"Pinecone Recommendations:\", recommended_movies)\n",
    "Using Weaviate:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def recommend_movies_weaviate(user_vector, k=5):\n",
    "    result = client.query.get('Movie', ['vector']) \\\n",
    "        .with_near_vector({'vector': user_vector.tolist()}) \\\n",
    "        .with_limit(k) \\\n",
    "        .do()\n",
    "    return result\n",
    "\n",
    "query_vector = create_user_vector(['movie1_id', 'movie2_id'])\n",
    "recommended_movies = recommend_movies_weaviate(query_vector)\n",
    "print(\"Weaviate Recommendations:\", recommended_movies)\n",
    "7. Recommendation Generation\n",
    "Objective: Integrate recommendations into the Netflix platform for user interaction.\n",
    "\n",
    "Display Recommendations:\n",
    "\n",
    "Show recommended movies on the user’s homepage or recommendations page.\n",
    "Personalized Suggestions:\n",
    "\n",
    "Use the recommendations to suggest movies based on user’s viewing history and preferences.\n",
    "Continuous Learning:\n",
    "\n",
    "Regularly update user profiles and movie embeddings based on new interactions and feedback to improve recommendations.\n",
    "Summary\n",
    "Using Hugging Face’s NLP models for generating high-quality embeddings, combined with vector databases like FAISS, Pinecone, or Weaviate, allows you to build a robust and scalable recommendation system. This system can efficiently handle large datasets, provide real-time recommendations, and deliver a highly personalized user experience by leveraging semantic understanding of movie descriptions and user preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63702f0-f20c-4dca-a7be-4bc67ed6316a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7f9de2-7d70-44a3-ab96-38c21aace648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
